30 Example: deck of cards

from collections import namedtuple
Card_tuple_subclass = namedtuple('Card_display_name', 'rank suit')

def __getitem__(self, position):
    """allows evaluation of deck[0], deck[-1], etc."""
    return self._cards[position]

31 random.choice picks an element from an array, range, or the deck, which implements __getitem__

33 len in CPython actually reads ob_size of a C struct. It's much faster, so it's the reason len is a function, not a method

35 __abs__, __add__, __mul__, __repr__

38 https://www.python.org/dev/peps/pep-0357/
__index__ allows any object to be used for slicing (in particular, NumPy uses different sized integers, that should be allowed inside slices)

mutable: list, deque, bytearray, array.array, memoryview
immutable: tuple, str, bytes

49 Cartesian product

colors = ['red', 'blue']
sizes = ['S', 'M', 'L']
tshirts = [(size, color) for size in sizes for color in colors]
# [('S', 'red'), ('S', 'blue'), ('M', 'red')...]

54 unpacking a tuple as arguments of a function

t = (20, 8)
quotient, remainder = divmod(*t)

55 use * to capture a variable amount of arguments. The starred variable may be anywhere. Unpacking structures may also be nested, such as (a, b, (c, d))

a, *b, c = range(5)  # b is [1, 2, 3]

64 Use [['_'] * 3 for i in range(3)] instead of [['_'] *  3] * 3 because the latter will repeat the same list reference three times
69 sorted() has two optional parameters: reverse=True and key=str.lower, len, etc
70 key is also used in min(), max(), itertools.groupby(), heapq.nlargest()

72 bisect.bisect(lst, x) is an alias for bisect_right() and returns an index where x should be inserted in lst, such as lst.insert(idx, x).

83 deque.rotate(n) : if n > 0, items are shifted to the right, otherwise to the left if n < 0. Items are moved in-place

heapq provides functions to operate on lists; call heapify(lst)

91 trick to sort a mixed list of strings and ints: sorted(lst, key=int)

94 use isinstance(3, int) to check the type of an object

ways of creating dicts:
a = dict(one=1, two=2)
b = {'one': 1, 'two': 2}
dict comprehensions { key: value for... }

97 d.get(key, [default])
d.setdefault(key, [default])
d.keys()
d.values()
d.update(m)

101 dd = defaultdict(list)  # call with a default factory (without arguments)

105 collections.OrderedDict
collections.ChainMap
collections.Counter
collections.UserDict

106 Subclassing UserDict is preferable to dict because certain overwritten dict methods do not get called. The actual data is stored in self.data

113 Set comprehensions {chr(i).upper() for i in range(48, 128)}
s.discard(e)  # does not raise an error if e does not exist
s.remove(e)   # raises KeyError if e does not exist

132 seq[0] == seq[:1] is only true for str. For other sequence types, seq[:1] will return the same sequence type, while seq[0] is of an item type

175 Functions are first-class objects (for example, round() is of <class 'builtin_function_or_method'>. User functions are <class 'function'>)
177 A higher-order function is one that accepts functions as arguments or returns a function
179 all(iterable) checks whether all values are True (or truthy), any(iterable) checks if at least one is truthy
180 a lambda expression's body is limited to pure expressions (assignments, while, try are not allowed)

182 When creating an instance that accepts a list, it's a good idea to make a local copy: self._items = list(items). This creates a shallow copy.

185 * and ** in function parameters
187 required keyword parameters may be placed after a single * in the parameter list: def f(a, *, b): ...

195 the 'operator' module includes many arithmetic operators that are more readable than an equivalent lambda expression
from functools import reduce
from operator import mul

def fact(n):
    return reduce(mul, range(1, n+1))

195 operator.itemgetter(i, j, ..., k) work like lst[i], returning the value of the ith item (or a tuple if there are many indices)
It can be used as the key(s) in sorting with respect to one or more columns

a = [0, 10, 20]
ig = itemgetter(0, 2)
ig(a)  # (0, 20)

sorted(b, key=itemgetter(1))

196 attrgetter extract attributes (fields) from objects

197 methodcaller(f, g, ...) calls the methods f, g, etc. on the given object argument

198 functools.partial creates a new function with some of the arguments of the original function fixed (frozen in place)
def f(a, b):
    return a + b

addTo9 = partial(f, 9)

addTo9(1)  # 10

223 a decorator is a callable that accepts another function as an argument (the decorated function). It returns a function. Classes may also be decorated

@decorate
def target():
    print("running target")

is the same as

def target():
    print("running target")
target = decorate(target)

224 decorators are run when a module is loaded (import time), and not at run time
222 nonlocal is often needed for writing custom decorators

229 variables that are assigned in a function are considered local
230 a variable can be declared global in a function
232 a closure is a function with an extended scope, enclosing non-global variables not defined in its body

234 variables not associated in the local scope are "free variables"

235 the only scenario where a function uses external, non-global variables is when it is nested in another function
236 because variable assignments can only be done with local variables, the nonlocal declaration is needed in creating closures

239 @functools.wraps copies relevant attributes from the decorated function to the decorator

240 @functools.lru_cache() implements memoization. It stands for Least Recently Used (a limited cache where items not recently used are discarded)

244 there is no function or method overloading in Python

258 variables are labels, not boxes (if you put the same list in two boxes, changing one will in fact affect both)
258 a variable is assigned to an object, and not the other way around (or bound)

260 several variables assigned to the same object are aliases
262 'is' and 'is not' check identity, while == checks the values
262 to check for None, use 'x is None' or 'x is not None'

264 lst2 = list(lst1) and lst1[:] creates shallow copies (only references are copied)
267 copy.deepcopy() creates deep copies. copy.copy creates shallow copies

269 in Python function calls, the parameters turn into aliases of the arguments (call by sharing). Arguments are passed using "call by object reference" (technically call by value, where the value is always an object reference)

immutable arguments are not changed, while mutable ones can be altered in place. However, they cannot be reassigned
Mnemonic: if the argument can be mutated, its identity will remain and the mutation will occur. Otherwise, a new object is created.

274 del only removes names, not objects

280 calling copy() or constructors of immutable types return the same object

282 Assigning a new value to an existing variable does not change the previously associated object. It is merely "rebinding"

293 for a @classmethod, cls is used as the first parameter (instead of self for regular methods)
293 classmethods are typically used as alternative constructors

293 a @staticmethod does not accept a special first parameter. It is like a regular function that happens to be inside a class definition

300 a custom __hash__ function can be the XOR (^) of the hashes of its components

310 class attributes can be used as default values for instance attributes

311 subclassing to override a class attribute is more commonplace than altering the target class' class attribute directly

class ShortVector2d(Vector2d):
    typecode = 'f'  # 'd' in original Vector2d

315 Python's approach to attributes is the opposite of Java's: in Java, a public attribute cannot later be implemented as getters and setters without breaking existing code
In Python, public attributes can later be further controlled with properties (which work as getters and setters)

322 the Python sequence protocol implies only __len__ and __getitem__
323 to support iteration, only __getitem__ is required

334 when using functools.reduce() for calculating hashes, the optional third parameter (initializer) should be 0, to avoid a TypeError with empty sequences

336 zip is lazy so it is perfect for checking pairs (or larger tuples) of iterables
if we assign z = zip(a, b, c), zip(*z) will reverse the operation

344 itertools.chain(*iterables) concatenates its arguments

352 An abstract class represents an interface (C++)

353 Generally speaking, creating custom ABCs (Abstract Base Classes) is only done by framework creators
353 a class' interface is its public and inherited attributes (data and methods), including dunder methods such as __getitem__

354 protocols are independent of inheritance; a class can implement many protocols

355 the ABC Sequence contains __getitem__, __contains__, __iter__, __reversed__, index, and count. It inherits methods from the superclasses Container (__contains__), Iterable (__iter__), and Sized (__len__)

371 @abc.abstractmethod marks an abstract method. Its body is empty except for a docstring (don't write anything, not even a pass or ...)
372 an ABC may have concrete methods. They should only count on the interface defined by the ABC
372 an abstract method may have an implementation, even if subclasses override them. To use them, use super()

375 @abstractmethod should be the innermost decorator (closest to def)

381 __subclasses__() returns a list of direct subclasses and _abc_registry is a data attribute associated with a WeakSet with weakrefs to virtual subclasses registered with the ABC
385 __subclasshook__() may cause a custom class to be recognized as a subclass of an ABC. For example, a class defining __len__ will be a subclass of abc.Sized

391 Strong vs. Weak typing: strong typing does not implicitly convert between types
Static vs. Dynamic typing: static typing checks types during compile time. Dynamic typing does so at run time

397 subclassing builtins is error-prone. Subclass UserDict, UserList, and UserString instead

400 a superclass' method may be called directly, passing the instance as the argument explicitly

401 The Method Resolution Order (__mro__) defines the specific order when traversing the inheritance graph

401 super() is safer and more flexible if there are future changes

Interfaces should explicitly be coded as ABCs

Classes designed to offer methods for unrelated subclasses should be mixin classes. Mixins should never be instantiated and their names should end in "Mixin". Concrete classes should inherit from more than one mixin

ABC concrete methods should only depend on its own (and superclasses') methods
Concrete classes should have only at most one other concrete superclass. The others should be ABCs or mixins

Composition is typically more flexible than inheritance

429 it's better to capture TypeErrors and return NotImplemented. The interpreter can then try to call the reversed operator

430 example of goose typing: check that a number isinstance of numbers.Real

435 if in doubt when implementing an operation or comparison, return NotImplemented

446 operator overloading makes mathematical formulas more readable, such as (1+rate) ** periods, instead of BigDecimal.ONE.add(rate).pow(periods) in Java

450 every generator is an iterator. An iterator retrieves items from a collection, while generators can create them on demand

454 try calling iter(x) and handle a TypeError if it is raised, instead of isinstance(x, abc.Iterable)
455 an iterable is any object from which iter() can obtain an iterator. Objects that implement __iter__ (that returns an iterator) is an iterable

Sequences are always iterable, as well as objects that implement __getitem__, counting from 0

456 the default interface of an iterator is:

__next__ returns the next item, or raises StopIteration if there are none left
__iter__ returns self, for use in places like for loops that expect iterables

458 an exhausted iterator must be recreated again. iter(it) returns self

461 iterators are iterable, but iterables are not iterators. Do not make an iterable an iterator also

461 a Pythonic implementation of an iterable uses a generator function to replace a custom iterator. (example of a Sentence class)

def __iter__(self):
    for word in self.words:
        yield word

472 if the main purpose of a class is to implement __iter__, it can be reduced to a generator function

473 itertools.count is a generator that returns numbers given a starting point and step

itertools.takewhile returns a generator given a predicate and another generator. Results are returned until the predicate is False
takewhile(lambda n: n < 5, count(1))  # <1, 2, 3, 4>

475 compress(it, selector) selectively picks items of it
compress('Aardvark', (1,0,1,1,0,1))  # ['A', 'r', 'd', 'a']
dropwhile(predicate, it) discards elements until predicate is False. takewhile returns elements until predicate is False
filter(pred, it) returns elements satisfying predicate. filterfalse is the opposite
islice(it, stop) or islice(it, start, stop, step=1) is similar to regular slices but can be used with any iterable and is lazy
accumulate(it, [func]) produces accumulated sums, or applies func (if given) to the first pair of items and then each item in the iterable
enumerate(it, start=0) creates tuple pairs (idx, item), (idx+1, next_item), and so on
map(func, it, [it2, ..., itn]) applies func to each item of it. If several iterables are given, func must accept that number of arguments
starmap(func, it) applies func(*iit) to items iit produced by it
chain(it1, ..., itN) concatenates the given iterables
chain.from_iterable(it) produces all items from each iterable created by it. it should produce a series of iterables, such as a list of iterables
product(it1, ..., itN, repeat=1) is the Cartesian product, with iterables repeated the given number of times. This is like creating sequences from nested for loops
zip(it1, ..., itN) produces tuples from the N iterables in parallel. Stops when the shortest iterable runs out
zip_longest(it1, ..., itN, fillvalue=None) fills gaps with fillvalue
combinations(it, out_len)
combinations_with_replacement(it, out_len)
cycle(it) repeatedly generates elements of it
permutations(it, out_len=None) by default, out_len=len(list(it))
repeat(object, [times]) if times is not given, the object is repeated endlessly
groupby(it, key=None) creates tuples (key, group) where group is a generator that produces items matching the key
reversed(seq) works on sequences or classes implementing __reversed__
tee(it, n=2) returns a tuple of n independent generators
g1, g2 = tee('ABC')

491 the value passed to the .send() method will be the value of the corresponding yield expression in the generator. Bidirectional data transfer is possible

- generators produce data for iteration
- coroutines are data consumers
- coroutines are not related to iteration

496 in one point of view, generators are iterators because they implement __next__ and __iter__. But iterators are not generators

499 'with' defines a temporary context and undoes it under the control of a context manager. It is commonly used to open files and close them automatically

500 'else' after a 'for' will be executed if the loop reaches its end (that is, it wasn't interrupted by a 'break')
'else' after 'while' executes similarly, if the loop wasn't ended by a 'break'
'else' after 'try' executes if no exception was raised
these 'else's are also ignored if an exception, 'return', 'break', or 'continue' exits the command's block

502 context managers exist to control 'with' blocks, just as iterators control 'for' loops

'with' simplifies the try/finally pattern
the context manager protocol is made up of __enter__ and __exit__
503 the context manager object is the result of evaluating the expression after with, but the value associated to the target variable is the result of the call to __enter__ in the context manager

506 some examples of context managers are:

- sqlite3 transactions
- threading locks and conditions
- Decimal localcontexts
- unittest.mock.patch

507 contextlib.closing creates context managers out of objects implementing close()
@contextlib.contextmanager creates a context manager out of a simple generator function
ContextDecorator is a base class that defines context managers from a class that can also be used as a function decorator, executing it in a managed context
ExitStack calls the stacked __exit__ methods of multiple context managers

509-510 error handling should be done with the yield inside a try block

try:
    yield "JABBERWOCKY"
except ZeroDivisionError:
    msg = "Do not divide by zero"
finally:
    sys.stdout.write = original_write
    if msg:
        print(msg)

510 you must explicitly reraise an exception in the decorated function if you don't want @contextmanager to suppress it

515 normally, a caller sends values to a coroutine with co.send(datum). Usually, yield appears on the right side of an assignment (datum = yield)

coroutines enable cooperative multitasking. each coroutine yields control to a central scheduler so other coroutines may be activated

with respect to coroutines, yield is used for flow control

516 using .send(), the data sent becomes the value of the yield in the generator function

.throw() and .close() allow raising an exception in the generator and terminating it

a generator may return a value with the 'return' statement

'yield from' simplifies nested generators

a simple coroutine must be started (also called priming) by calling next(coro) or coro.send(None), otherwise we get a TypeError (can't send non-None value to a just-started generator)

518 inspect.getgeneratorstate() returns a coroutine's one of 4 states:
GEN_CREATED, GEN_RUNNING, GEN_SUSPENDED, or GEN_CLOSED

520 execution stops exactly at the yield expression. Remember that in assignment statements, the right side is evaluated first

---.
b = \ yield a  # this side is evaluated first
     `--------

'a' is returned to the caller, and b will only be assigned after .send() is called

525 coro.throw(SomeException) makes the yield raise the given Exception. It may be handled inside the coroutine
coro.close() raises GeneratorExit and halts the coroutine

527 if some cleanup is necessary independently of how the coroutine stops, try/finally blocks are needed

530 when a generator calls 'yield from subgen()', subgen takes control and returns values to whoever called gen. It is like calling subgen directly. While subgen runs, gen is blocked

yield from 'AB' can replace for c in 'AB': yield c

531 the true power of yield from is to open a bidirectional channel between the outermost caller and the innermost subgenerator, without much code in between

532 in 'yield from' contexts, a delegating generator is the generator containing the 'yield from <iterable>' expression

the 'subgenerator' is the generator obtained from <iterable>

the 'caller' is the client code that calls the delegating generator

533 the averager_count() coroutine is the subgenerator. The delegating generator is:

def grouper(results, key):
    while True:
        results[key] = yield from averager_count()

the client is:

results = {}
for key, values in data.items():
    group = grouper(results, key)
    next(group)
    for value in values:
        group.send(value)
    group.send(None)

537 any value produced by the subgenerator is passed directly to the client code

any value sent with .send() is passed to the subgenerator. if None is sent, the subgenerator's __next__() is called. If StopIteration is raised, the delegating generator resumes execution. Other exceptions are propagated to the delegating generator

return expr raises StopIteration(expr) on the generator's exit

Exceptions (except GeneratorExit) thrown in the delegating generator using .throw() are passed to the subgenerator's .throw()

If GeneratorExit or .close() is called in the delegating generator, it is also called in the subgenerator

549 queue.PriorityQueue is used for storing events

560 futures are objects that represent the asynchronous execution of an operation

565 the main resources of concurrent.futures are ThreadPoolExecutor and ProcessPoolExecutor. They implement an interface allowing you to submit callables to be executed in separate threads or processes

from concurrent import futures

workers = min(MAX_WORKERS, len(country_list))
with futures.ThreadPoolExecutor(workers) as executor:
    res = executor.map(download_one, country_list)
    return len(list(res))

567 there are two Future classes: concurrent.futures.Future and asyncio.Future. They have the same purpose: to represent a delayed process that may or may not have been concluded

Future is similar to Promise in JavaScript

Futures encapsulate pending operations so they may be placed in queues and their results and inspect their results when available

We don't need to create Futures. Passing a callable to Executor.submit() will create one automatically

Both Future objects have a .done() method that is non-blocking and returns a Boolean telling whether the associated callable executed or not

a callable passed to .add_done_callback() will be called with the Future as the only argument when it has been executed

.result() returns the result of the callable, reraises an Exception. In concurrent.futures.Future, f.result() blocks. An optional 'timeout' value can be passed, which will raise TimeoutError if the delayed process still hasn't returned

Though .result() exists in asyncio.Future, it is preferable to use yield from
569 if future.result() is inside as_completed(), it will not block because as_completed() only yields completed futures

570 none of the given scripts so far download in parallel (only one thread is active)
CPython's internal data structures are not thread-safe, so the GIL (Global Interpreter Lock) allows only a single thread at a time to execute bytecode

It is not the language's limitation. Jython does not have this limitation
all standard library functions that block I/O free the GIL. Despite the existence of the GIL, Python threads are perfectly usable in I/O-bound applications

576 Executor.map returns the Futures' results in the same order as they were called. If the first Future takes 10 seconds to finish, and the others only one second, the results will be blocked

To have results shown as they arrive, a combination of Executor.submit() and futures.as_completed() is needed

582 'raise' by itself reraises the current Exception

with futures.ThreadPoolExecutor(max_workers=concur_req) as executor:

in executor.submit(download_one, cc, base_url, verbose), the first argument is the callable to be executed. The remaining arguments are passed to the callable

594 concurrency is about dealing with many things at the same time (structure)
parallelism is about doing many things at the same time (execution)

595 asyncio implements concurrency with coroutine driven by an event loop.

608 trick: the general logic of a coroutine can be better understood if we pretend yield froms are not there

the innermost coroutine at the end of the chain always delegate with yield from a method of asyncio or coroutines of libraries that implement higher-level protocols

611 the memory overhead per thread is in the order of megabytes, so it's infeasible to create one per connection

callbacks are the traditional method of implementing asynchronous calls. Instead of waiting for a response, we register a function to be called when something happens. In this manner, every call is non-blocking

such a framework depends on interrupts, polling, background processes, etc

from the event loop's point of view, calling a callback or .send() is practically the same

615 asyncio.Semaphore is a synchronization device that limits the number of concurrent requests
616 a Semaphore contains an internal counter which decrements on .acquire() and increments on .release(). If the counter is zero, acquire() blocks until another coroutine releases

with (yield from semaphore):

617 main calls download_many (a regular function), which instantiates the downloader_coro

618 we cannot map futures to country codes because internally, asyncio replaces supplied futures with others so that in the end the same results are provided. A custom FetchError encapsulates a network Exception and saves the country code for error reporting

619 internally, asyncio's event loop has an executor of a pool of threads, and you can send callables to be executed in it with run_in_executor

620 loop = asyncio.get_event_loop()
loop.run_in_executor(None, save_flag, image, cc.lower() + '.gif')

passing None means using the default pool of threads

622 callback nesting or chaining can be replaced by a coroutine with multiple yield froms

623 the price to pay for using a coroutine is the more complicated setup: loop.create_task(three_stages(request1))

626 use yield from with coroutines and asyncio.Future instances

630 asyncio.start_server()
loop.run_until_complete(server.wait_closed())

632 asyncio.streams has a ready-to-use server. We only need to implement a handler function

there is also a lower-level transports module

633 loop.create_server()

host = loop.run_until_complete(init(loop, address, port))

__getattr__ and __setattr__ are called to evaluate dot notation expressions (obj.attr)

649 __getattr__ is called only if there was no attribute found (in the instance, class, or superclasses)

659 properties are class attributes designed to manage instance attributes

661 an empty class doesn't need pass, but it's good to add a docstring

664 'factory' to instantiate an object of the right class

666 property with validation

@property  # getter
def weight(self):
    return self.__weight

@weight.setter
def weight(self, value):
    if value > 0:
        self.__weight = value
    else:
        raise ValueError("weight must be positive")

669 properties override instance attributes

670 a property is a class attribute, but is returned instead of an instance attribute. If the property is replaced, the instance attribute will be returned

672 a property's docstring should be stored in the getter

673 factory for creating 'quantity' properties

674 instance.__dict__[storage_name] is used to avoid an infinite loop

676 @my_property.deleter is used to encapsulate the method responsible for deleting the attribute managed by the property

678 obj.__class__ is the same as type(obj)
__dict__ holds the writeable attributes
__slots__ may be defined in a class to limit the attributes its instances can have

dir(), getattr(), setattr(), hasattr(), vars()

__delattr__, __dir__, __getattr__, __getattribute__, __setattr__

__getattribute__ is called before __getattr__. __getattr__ is called when an attribute is not found

684 in Python, a function call and instantiation both look the same. A factory may be better than a constructor

687 Descriptors are a way of reusing the same logic when accessing several attributes

a descriptor is a class that implements a protocol made up of __get__, __set__, and __delete__. Most descriptors do not implement all three

688 the properties factory uses a functional programming approach, while descriptors is object-oriented

a descriptor is used in a different class by declaring instances of the descriptor as class attributes

in the LineItem class, there are two 'weight's, one is an instance attribute and the other is a class attribute

692 descriptors are based on a protocol. There is no need to make it a subclass

718 an example of metaprogramming is creatting a new class with a function (without using 'class')

metaclasses allow the creation of new categories of classes with special characteristics, such as ABCs (Abstract Base Classes)

719 record_factory is inspired by namedtuple, and creates simple classes on demand

721 type(cls_name, (object,), cls_attrs) creates a new type

726 in 'import time', the bodies of all classes are executed

730 a metaclass is written like a class

type is an instance of type (of itself). These relationships are all 'magical'

ABCMeta and Enum are metaclasses

ABCMeta is an instance and subclass of type. Being a subclass of type, ABCMeta inherits the ability to create classes

737 class EntityMeta(type):

class Entity(metaclass=EntityMeta):
    """Business entity with validated fields"""

739 __prepare__ is called before __new__ and it creates the mapping used to fill in the class body's attributes

@classmethod
def __prepare__(cls, name, bases):
    return collections.OrderedDict()
